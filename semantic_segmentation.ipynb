{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# セマンティックセグメンテーション"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 今回のタスク\n",
    "セマンティックセグメンテーション（Semantic Segmentation）<br>\n",
    "画像のピクセル（画素）1つひとつに対してラベル付けしていく手法<br>\n",
    "<img src=\"img/semantic_segmentation.png\" width=\"800\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用するモデル：UNet\n",
    "- 医療画像解析に特に強みを持つエンコーダ・デコーダ構造のネットワークです。\n",
    "- 特徴マップの空間情報を保持しながら高精度なセグメンテーションが可能で、様々な分野で利用されています。\n",
    "- シンプルな構造で拡張が容易なため、多クラスセグメンテーションにも適用されています。\n",
    "<img src=\"img/unet.png\" width=\"800\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失関数：Dice Cross-Entropy Loss\n",
    "- ダイス損失（Dice Loss）とクロスエントロピー損失を足し合わせた損失関数\n",
    "$$Dice Loss = 1 - \\frac{2|A \\cap B|}{|A| + |B|}$$\n",
    "<img src='img/dice_loss.png' width=800></img><br>\n",
    "$$Cross Entropy Loss = -\\frac{1}{N}\\sum_{i=1}^Np(x)\\log{q(x)}$$\n",
    "<img src='img/cross_entropy.png' width=800></img><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 準備"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ライブラリのインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from unet import UNet\n",
    "from dataset import VOCDataset\n",
    "from util import DiceCrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カラーパレット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOC2012で用いるラベル\n",
    "CLASSES = ['backgrounds','aeroplane','bicycle','bird','boat','bottle',\n",
    "            'bus','car' ,'cat','chair','cow', \n",
    "            'diningtable','dog','horse','motorbike','person', \n",
    "            'potted plant', 'sheep', 'sofa', 'train', 'monitor','unlabeld'\n",
    "            ]\n",
    "\n",
    "# カラーパレットの作成\n",
    "COLOR_PALETTE = np.array(Image.open(\"./VOCdevkit/VOC2012_sample/SegmentationClass/2007_000170.png\").getpalette()).reshape(-1,3)\n",
    "COLOR_PALETTE = COLOR_PALETTE.tolist()[:len(CLASSES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# カラーパレットの可視化\n",
    "fig, axes_list = plt.subplots(len(CLASSES), 1, figsize=(5, 10))\n",
    "for i, color in enumerate(COLOR_PALETTE[:len(CLASSES)]):\n",
    "    color_img = np.full((1, 10, 3), color, dtype=np.uint8)\n",
    "\n",
    "    axes_list[i].imshow(color_img, aspect='auto')\n",
    "    axes_list[i].set_axis_off()\n",
    "    axes_list[i].text(-1, 0, f'{i}: {CLASSES[i]}', va='center', ha='right', fontsize=10)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "今回はこのカラーパレットの通りに識別ができるように訓練を行います。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 画像サンプル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力画像・出力画像の例\n",
    "input = Image.open('VOCdevkit/VOC2012_sample/JPEGImages/2007_000032.jpg')\n",
    "gt = Image.open('VOCdevkit/VOC2012_sample/SegmentationClass/2007_000032.png')\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "fig.add_subplot(2,3,1).set_title('input')\n",
    "plt.imshow(input)\n",
    "fig.add_subplot(2,3,2).set_title('gt')\n",
    "plt.imshow(gt)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット・データローダー"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データのリストファイル\n",
    "train_list_path = 'VOCdevkit/VOC2012_sample/listfile/train_list_300.txt'\n",
    "val_list_path = 'VOCdevkit/VOC2012_sample/listfile/val_list_100.txt'\n",
    "\n",
    "# データディレクトリ\n",
    "img_dir = 'VOCdevkit/VOC2012_sample/JPEGImages'\n",
    "gt_dir = 'VOCdevkit/VOC2012_sample/SegmentationClass'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# リストファイルの読み込み\n",
    "with open(train_list_path, 'r') as f:\n",
    "    train_list = f.read().splitlines()\n",
    "with open(val_list_path, 'r') as g:\n",
    "    val_list = g.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "train_ds = VOCDataset(img_list=train_list, img_dir=img_dir, gt_dir=gt_dir, data_augmentation=False)\n",
    "val_ds = VOCDataset(img_list=val_list, img_dir=img_dir, gt_dir=gt_dir)\n",
    "print(f\"len(train_data): {train_ds.__len__()}\")\n",
    "print(f\"len(test_data): {val_ds.__len__()}\")\n",
    "\n",
    "# データローダーの作成\n",
    "train_dl = DataLoader(train_ds, batch_size=16, num_workers=8, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=16, num_workers=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練\n",
    "def train(dataloader, model, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    step_total = math.ceil(size/batch_size)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with tqdm.tqdm(enumerate(dataloader), total=step_total) as pbar:\n",
    "        for batch, item in pbar:\n",
    "            inp, gt = item[0].to(device), item[1].to(device)\n",
    "\n",
    "            # 推論\n",
    "            pred = model(inp)\n",
    "\n",
    "            # 損失誤差を計算\n",
    "            loss = criterion(pred, gt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # バックプロパゲーション\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # プログレスバーに損失を表示\n",
    "            pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / step_total\n",
    "        print(f\"Train Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# テスト\n",
    "def test(dataloader, model, criterion, device):\n",
    "    model.eval()\n",
    "    size = len(dataloader.dataset)\n",
    "    batch_size = dataloader.batch_size\n",
    "    step_total = math.ceil(size/batch_size)\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for item in dataloader:\n",
    "            inp, gt = item[0].to(device), item[1].to(device)\n",
    "            pred = model(inp)\n",
    "            loss = criterion(pred, gt)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / step_total\n",
    "        print(f\"Val Loss: {avg_loss:.4f}\")\n",
    "    return avg_loss\n",
    "\n",
    "# 推論結果の可視化\n",
    "def visualize(model, img_id, img_dir, gt_dir, criterion, device='cpu'):\n",
    "    img_path = os.path.join(img_dir, img_id) + \".jpg\"\n",
    "    gt_path = os.path.join(gt_dir, img_id) + \".png\"\n",
    "    class_values = [CLASSES.index(cls.lower()) for cls in CLASSES]\n",
    "    \n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # 入力画像の前処理 (PIL画像 -> Tensor)\n",
    "        inp_tensor = transforms.functional.to_tensor(Image.open(img_path))\n",
    "        inp_resized = transforms.Resize(size=(256, 256))(inp_tensor)\n",
    "        inp_normalized = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(inp_resized)\n",
    "        inp = inp_normalized.unsqueeze(0)\n",
    "\n",
    "        # 正解画像の前処理 (PIL画像 -> NumPy -> Tensor)\n",
    "        gt_np = np.asarray(Image.open(gt_path))\n",
    "        gt_np = np.where(gt_np == 255, 0, gt_np)  # 範囲外の255を0に置換（または別のインデックスに）\n",
    "\n",
    "        gt_tensor = torch.tensor(gt_np, dtype=torch.long)\n",
    "        gt_tensor = gt_tensor.unsqueeze(0)\n",
    "        gt_resized = transforms.Resize(size=(256, 256))(gt_tensor)\n",
    "\n",
    "        # 予測\n",
    "        pred = model(inp)\n",
    "        # loss = criterion(pred, gt_resized) # loss計算\n",
    "        # print(f'Loss: {loss.item()}')\n",
    "\n",
    "        inp_np = (inp_resized.numpy().transpose(1, 2, 0) * 255).astype(np.uint8)  # 表示用スケール\n",
    "        pred_np = torch.argmax(pred, dim=1).cpu().detach().numpy()[0]\n",
    "        pred_np = np.clip(pred_np, 0, len(COLOR_PALETTE) - 1).astype(np.int32)\n",
    "        gt_resized = gt_resized.squeeze(0).detach().numpy().astype(np.int32)\n",
    "\n",
    "        # カラーパレットの適用\n",
    "        img_gt = np.array([[COLOR_PALETTE[gt_resized[i, j]] for j in range(256)] for i in range(256)], dtype=np.uint8)\n",
    "        img_pred = np.array([[COLOR_PALETTE[pred_np[i, j]] for j in range(256)] for i in range(256)], dtype=np.uint8)\n",
    "\n",
    "    # プロット\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    for i, im in enumerate([inp_np, img_gt, img_pred]):\n",
    "        ax = fig.add_subplot(1, 3, i+1)\n",
    "        ax.imshow(im)\n",
    "        ax.set_title([\"Input\", \"Ground Truth\", \"Prediction\"][i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 訓練"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 設定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_epoch = 0 # ここは変えない\n",
    "epochs = 10 # エポック\n",
    "lr = 0.01 # 学習率\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else \"cpu\" # デバイスの設定\n",
    "print(\"Using {} device\".format(device))\n",
    "\n",
    "model = UNet() # モデル\n",
    "model = model.to(device)\n",
    "\n",
    "weight = None # torch.tensor([0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.5]) # クロスエントロピー損失のクラスごとの重み\n",
    "dice_weight = 0.5 # 損失のうち、Dice Lossの比率\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) # 最適化手法\n",
    "\n",
    "save_model_n = None # 5 # 重みを保存する頻度（＊容量を圧迫する可能性があるので小さくしすぎないよう注意！！クラスターであれば気にしなくてOK。）\n",
    "save_dir = './your_save_dir' # 訓練ログ保存ディレクトリ\n",
    "save_name_prefix = 'your_save_name' # 訓練ログ保存名\n",
    "csv_path = os.path.join(save_dir, f'{save_name_prefix}.csv')\n",
    "\n",
    "chkp_path = None # 'weights/unet_241106_00.pth' # 重みの初期値\n",
    "continuation = False # 訓練を続きから再開する場合True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存ディレクトリの作成\n",
    "if not os.path.exists(save_dir):\n",
    "    os.mkdir(save_dir)\n",
    "\n",
    "# 重みの読み込み。\n",
    "if chkp_path is not None:\n",
    "    checkpoint = torch.load(chkp_path, map_location='cpu')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'], strict=False)\n",
    "    if continuation == True:\n",
    "        initial_epoch = checkpoint[\"epoch\"] + 1\n",
    "\n",
    "if weight is not None:\n",
    "    weight = weight.to(device)\n",
    "criterion =  DiceCrossEntropyLoss(weight=weight, dice_weight=dice_weight) # 損失関数\n",
    "\n",
    "for epoch in range(initial_epoch, epochs):\n",
    "    print(f\"Epoch {epoch}\\n-------------------------------\")\n",
    "    train_loss = train(train_dl, model, optimizer, criterion, device)\n",
    "    val_loss = test(val_dl, model, criterion, device)\n",
    "    if save_model_n is not None:\n",
    "        if epoch == save_model_n-1:\n",
    "            # モデルの重みを保存\n",
    "            torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                },\n",
    "            os.path.join(save_dir, f'{save_name_prefix}_ep{epoch}.pth'),\n",
    "            )\n",
    "            save_model_n += save_model_n\n",
    "    # 訓練ログ記入\n",
    "    if not os.path.exists(csv_path):\n",
    "        with open(csv_path, 'w') as f:\n",
    "            f.write('epoch,train_loss,val_loss\\n')\n",
    "    with open(csv_path, 'a') as f:\n",
    "        f.write(f'{epoch},{train_loss},{val_loss}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 可視化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 損失のプロット"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = pd.read_csv(csv_path)\n",
    "plt.plot(log_df['epoch'], log_df['train_loss'], label='trian_data')\n",
    "plt.plot(log_df['epoch'], log_df['val_loss'], label='test_data')\n",
    "plt.legend()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "# plt.ylim(0,2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 推論結果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>train data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000032'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000648'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_001225'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_002488'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>test data</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_000033'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_001763'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_id = '2007_003367'\n",
    "visualize(model, img_id, img_dir, gt_dir, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
